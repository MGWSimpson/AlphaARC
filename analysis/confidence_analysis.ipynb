{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66af9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from alphaarc.task import Task\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import json\n",
    "import torch, tqdm, pickle\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from alphaarc.task import Task\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "917c29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/split_keys.json') as fp:\n",
    "    json_object = json.load(fp)\n",
    "\n",
    "validation_tasks = json_object['val']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b507b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaarc.policy.tokenize import tokenize_task\n",
    "\n",
    "def encode_task(task, tokenizer, model, input_state_max=256, n_examples=10, max_length=256): \n",
    "    tokenized_task = np.array(tokenize_task(task, tokenizer, n_examples, input_state_max, max_length)['input_ids'])\n",
    "    return tokenized_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a80aee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, \\\n",
    "                            accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "\n",
    "# --- metrics.py -------------------------------------------------------------\n",
    "import torch\n",
    "\n",
    "def token_score(logits: torch.Tensor, ids: torch.Tensor | None = None,\n",
    "                kind: str = \"entropy\") -> torch.Tensor:\n",
    "   \n",
    "    log_p = torch.log_softmax(logits, -1)         # [1, L, V]\n",
    "    p      = log_p.exp()\n",
    "\n",
    "    if kind == \"entropy\":\n",
    "        score = -(p * log_p).sum(-1)              # [1, L]\n",
    "    elif kind == \"nll\":\n",
    "        if ids is None:\n",
    "            raise ValueError(\"ids required for kind='nll'\")\n",
    "        score = -log_p.gather(-1, ids.unsqueeze(0).unsqueeze(-1)).squeeze(-1)\n",
    "    elif kind == \"max_prob\":\n",
    "        score = 1.0 - p.max(-1).values            # 1 – confidence\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kind: {kind}\")\n",
    "\n",
    "    return score.squeeze(0).detach().cpu()        # [L]\n",
    "\n",
    "\n",
    "# --- eval.py ----------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def gather_val_records(val_tasks, tok, model, score_kind=\"entropy\",\n",
    "                       tau=0.5, batch_size=4,\n",
    "                       top_k=8,\n",
    "                       prob_mass=0.9,\n",
    "                       prob_thresh=0):\n",
    "    \"\"\"\n",
    "    Returns three parallel lists\n",
    "        entropy  : per-token entropy / score\n",
    "        is_error : 1 if token is wrong, else 0\n",
    "        is_confident_error : 1 if wrong *and* score < τ\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    score_lst, err_lst, conf_err_lst = [], [], []\n",
    "    error_free_tasks = []\n",
    "    \n",
    "    for task in tqdm.tqdm(val_tasks, desc=f\"val ({score_kind})\"):\n",
    "        src_ids = torch.tensor(encode_task(task, tok, model)).unsqueeze(0)\n",
    "        tgt_ids = tok(task.program_lines, return_tensors=\"pt\").input_ids\n",
    "        src_ids, tgt_ids = src_ids.to(model.device), tgt_ids.to(model.device)\n",
    "\n",
    "        logits = model(src_ids, labels=tgt_ids).logits        # [1, L, V]\n",
    "        score  = token_score(logits, tgt_ids, kind=score_kind)# [L]\n",
    "        #pred   = logits.argmax(-1).squeeze(0)                 # [L]\n",
    "        #wrong  = (pred != tgt_ids.squeeze(0)).float()         # [L]\n",
    "        \"\"\"k = max(1, top_k)                       # safety\n",
    "        topk_idx = logits.topk(k, dim=-1).indices          # [1, L, k]\n",
    "        tgt_exp  = tgt_ids.unsqueeze(-1)                   # [1, L, 1]\n",
    "        is_correct = (topk_idx == tgt_exp).any(-1).squeeze(0).float()  # [L]\n",
    "        wrong      = 1.0 - is_correct \"\"\"  \n",
    "        \n",
    "        \"\"\"prob_sorted, idx_sorted = logits.softmax(-1).sort(-1, descending=True)\n",
    "        cum_prob    = prob_sorted.cumsum(-1)                    # [1, L, V]\n",
    "        # first position where cumulative probability reaches the threshold\n",
    "        cutoff_idx  = (cum_prob >= prob_mass).float().argmax(-1)  # [1, L]\n",
    "        # rank (0-based) of the reference token in the sorted list\n",
    "        tgt_rank    = (idx_sorted == tgt_ids.unsqueeze(-1)).float().argmax(-1)\n",
    "        is_correct  = (tgt_rank <= cutoff_idx).squeeze(0).float()  # [L]\n",
    "        wrong       = 1.0 - is_correct\"\"\"\n",
    "        \"\"\"probs      = logits.softmax(-1)                              # [1, L, V]\n",
    "        tgt_prob   = probs.gather(-1, tgt_ids.unsqueeze(-1))         # [1, L, 1]\n",
    "        is_correct = (tgt_prob.squeeze(-1) >= prob_thresh).float()   # [1, L]\n",
    "        wrong      = 1.0 - is_correct.squeeze(0)   \n",
    "        \"\"\"\n",
    "        \n",
    "        k          = max(1, top_k)                                  # safety\n",
    "        probs      = logits.softmax(-1)                             # [1, L, V]\n",
    "\n",
    "        topk_prob, topk_idx = probs.topk(k, dim=-1)                 # [1, L, k]\n",
    "        tgt_exp    = tgt_ids.unsqueeze(-1)                          # [1, L, 1]\n",
    "\n",
    "        in_topk    = (topk_idx == tgt_exp).any(-1)                  # [1, L]\n",
    "        tgt_prob   = probs.gather(-1, tgt_exp).squeeze(-1)          # [1, L]\n",
    "\n",
    "        is_correct = (in_topk & (tgt_prob >= prob_thresh)).float()  # [1, L]\n",
    "        wrong      = 1.0 - is_correct.squeeze(0) \n",
    "\n",
    "        score_lst.extend(score.tolist())\n",
    "        err_lst.extend(wrong.tolist())\n",
    "        conf_error = ((score < tau) & wrong.bool()).float().tolist()\n",
    "        conf_err_lst.extend(conf_error)\n",
    "\n",
    "        if sum(conf_error) == 0:\n",
    "            error_free_tasks.append(task)\n",
    "\n",
    "    print(len(error_free_tasks))\n",
    "    return score_lst, err_lst, conf_err_lst\n",
    "\n",
    "\n",
    "def entropy_reliability(records, tau=0.5):\n",
    "\n",
    "\n",
    "    score, err, conf_err = map(torch.tensor, zip(*records))\n",
    "\n",
    "    n_err          = err.sum().item()\n",
    "    n_conf_err     = conf_err.sum().item()\n",
    "    frac_conf_err  = n_conf_err / max(n_err, 1)\n",
    "\n",
    "    print(f\"Total tokens         : {len(score):,}\")\n",
    "    print(f\"Errors               : {n_err:,}\")\n",
    "    print(f\"Confident errors (<τ): {n_conf_err:,}  \"\n",
    "          f\"({frac_conf_err*100:.1f}% of all errors with τ={tau})\\n\")\n",
    "\n",
    "    # --- diagnostic curves treat LOW score = high confidence, so invert sign\n",
    "    inv = -score.numpy()\n",
    "    auc_roc = roc_auc_score(err, inv)\n",
    "    prec, rec, thr = precision_recall_curve(err, inv)\n",
    "    pr_auc = auc(rec, prec)\n",
    "\n",
    "    print(f\"ROC-AUC (conf vs err): {auc_roc:.3f}\")\n",
    "    print(f\"PR-AUC               : {pr_auc:.3f}\")\n",
    "\n",
    "    # Simple two-way classification: “confident error” vs everything else\n",
    "    pred_err = score < tau\n",
    "    overall_acc = accuracy_score(err, pred_err)\n",
    "    tn, fp, fn, tp = confusion_matrix(err, pred_err).ravel()\n",
    "\n",
    "    print(f\"Overall accuracy     : {overall_acc:.3f}\")\n",
    "    print(f\"Specificity (correct): {tn / (tn+fp):.3f}\")\n",
    "    print(f\"Recall (errors)      : {tp / (tp+fn):.3f}\\n\")\n",
    "    print(classification_report(err, pred_err,\n",
    "                                target_names=[\"correct\", \"error\"], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "453c6949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val (entropy):   4%|▍         | 4/89 [00:00<00:07, 12.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "val (entropy): 100%|██████████| 89/89 [00:12<00:00,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "Total tokens         : 13,092\n",
      "Errors               : 655.0\n",
      "Confident errors (<τ): 53.0  (8.1% of all errors with τ=0.1)\n",
      "\n",
      "ROC-AUC (conf vs err): 0.057\n",
      "PR-AUC               : 0.026\n",
      "Overall accuracy     : 0.168\n",
      "Specificity (correct): 0.173\n",
      "Recall (errors)      : 0.081\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     correct      0.781     0.173     0.283     12437\n",
      "       error      0.005     0.081     0.010       655\n",
      "\n",
      "    accuracy                          0.168     13092\n",
      "   macro avg      0.393     0.127     0.146     13092\n",
      "weighted avg      0.742     0.168     0.270     13092\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('../finetune-checkpoint/dev-checkpoint')\n",
    "tok = AutoTokenizer.from_pretrained('Salesforce/codet5p-220m')\n",
    "\n",
    "val_tasks = []\n",
    "\n",
    "\n",
    "for file_name in json_object['val']:\n",
    "    task = Task.from_json(f'../data/training/{file_name}.json')\n",
    "    val_tasks.append(task )\n",
    " \n",
    "\n",
    "\n",
    "tau = 0.1\n",
    "score_kind = \"entropy\"          # or \"nll\" / \"max_prob\"\n",
    "\n",
    "score, err, conf_err = gather_val_records(\n",
    "        val_tasks, tok, model, score_kind=score_kind, tau=tau)\n",
    "\n",
    "entropy_reliability(list(zip(score, err, conf_err)), tau=tau)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaARC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
